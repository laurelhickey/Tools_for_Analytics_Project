{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "# !pip install matplotlib\n",
    "# !pip install requests\n",
    "# !pip install bs4\n",
    "# !pip install sqlalchemy\n",
    "# !pip install pandas\n",
    "# !pip install geojsonio --upgrade\n",
    "# !pip install geopandas\n",
    "\n",
    "# !pip install ipywidgets\n",
    "# !pip install keplergl\n",
    "# !jupyter nbextension install --py --sys-prefix keplergl\n",
    "import pandas as pd\n",
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "import math\n",
    "from math import *\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "import datetime\n",
    "import geojsonio\n",
    "import numpy as np\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "# weather csv data file\n",
    "# 2009_weather to 2015_weather (just pick the first 6 months)\n",
    "csv__= '/Users/morax/Documents/哥大/IEORE4501/IEOR4501 HW/IEOR4501 Project/'\n",
    "csv09_file = csv__ + '2009_weather.csv'\n",
    "csv10_file = csv__ + '2010_weather.csv'\n",
    "csv11_file = csv__ + '2011_weather.csv'\n",
    "csv12_file = csv__ + '2012_weather.csv'\n",
    "csv13_file = csv__ + '2013_weather.csv'\n",
    "csv14_file = csv__ + '2014_weather.csv'\n",
    "csv15_file = csv__ + '2015_weather.csv'\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "In this section in order to calculate the distance between two points in the uber data, we must use the longitude and latitude of the pickup and drop off locations.  Therefore, by using math module in order to calculate the distance between these two coordinates is given in the function calculate_distance().  Two different functions are written below using two different methods of calculating distance using the math module both with varying degrees of accuracy, in some situations the first method is more accurate than the second while in others the reverse is true.  It is important to note that, there are more accurate ways to calculate the distance based upon longitude and latitude that do not use just the math module.  In addtion, in the taxi data sets, the distance is already calculated, however, it is given in miles, so the miles_to_km() function converts the miles to kilometers so that a direct comparison between taxis and ubers can be made.  It should also be noted that the taxi distance is a distance driving on city streets whereas the uber distance is just a birdseye view distance, therefore, the distances in the taxi data set are likely to be slightly longer.  The function, add_distance_column() can be used in order to add the calculated distance to the uber data set.  This function uses the pickup and drop off longitudes and latitudes in order to build the additional column row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    # Longitude is x, Latitude is y, \n",
    "    # Longitude x\n",
    "    long = (to_coord[0]-from_coord[0])*40000*math.cos((to_coord[1]+from_coord[1])*math.pi/360)/360\n",
    "    # Latitude y\n",
    "    lat = (to_coord[1]-from_coord[1])*40000/360\n",
    "    # so the distance is just the side z followed by x^2+y^2=z^2\n",
    "    distance = sqrt(long*long+lat*lat)\n",
    "    return distance\n",
    "\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "def calculate_distance2(from_coord, to_coord):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    # Longitude is x, Latitude is y, \n",
    "    # math.radians() converts a degree value into radians. \n",
    "    lon1 = radians(from_coord[0])\n",
    "    lat1 = radians(from_coord[1])\n",
    "    lon2 = radians(to_coord[0])\n",
    "    lat2 = radians(to_coord[1])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# well sometime calculate_distance is better than calculate_distance2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc07e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miles_to_km(distance_miles):\n",
    "    distance_km = distance_miles /0.62137119\n",
    "    return distance_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fix the variable names called in lambda function\n",
    "def add_distance_column(dataframe): \n",
    "    dataframe['Distance'] = dataframe.apply(lambda x: calculate_distance((x['pickup_longitude'], x[\"pickup_latitude\"]), (x['dropoff_longitude'], x['dropoff_latitude'])),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "In this section, the taxi data set is being processed, the first step in processing the taxi data is first finding the links, then once those links are found they must be read in, and processed by combining each dataset for each month and year into one larger dataset and changing the distance column from miles to kilometers.\n",
    "\n",
    "-**get_taxi_html() and find_taxi_parquet_urls():** These functions are implementing web scraping in order to find the links for each data set of yellow taxi cabs. The first function, get_taxi_html(), is returning the html content of the web page that has the Taxi data. The second function, find_taxi_parquet_links is from the web page pulling out all of the links, then iterating through those links to see which are datasets for yellow taxi cabs from January 2009 to June 2015.\n",
    "\n",
    "-**get_and_clean_month_taxi_data(url)** This function reads in the data for each month of each year.  In addition, this funciton also takes a sample of the dataset to match the size of the Uber's dataset.  This means that as the Uber dataset has 1.8 million data points if it is assumed that those data points are evenly distributed across the 78 months, each of the taxi datasets should have approximatly 23076.9 rows in it, therefore, each parquet file is randomly sampled to pick out 23080 rows.\n",
    "\n",
    "-**zones_within_bbox()** This is a function that uses the taxi zone json file and using the centriod of the zone determines if that zone is within the bounding box (http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047).  If the zone is within the box it is included in the output list which is used later to remove data from some of the taxi files that do not have latitude and longitude included in their data.\n",
    "\n",
    "-**get_and_clean_taxi_data()** This function concatinates all of the data for each month of each year into one large data set. In addition, it cleans the dataframe data types, removes any points that are outside of the bounding box (http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047) that is essentially the bounds of NYC and normalizes the column names in the files that are from 2009 to 2010 and 2011 to 2015 as they have slightly different names and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_html():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    # check if the request was succeeded\n",
    "    if not response.status_code == 200:\n",
    "        return None\n",
    "    return html\n",
    "\n",
    "\n",
    "def find_taxi_parquet_urls():\n",
    "    soup = bs4.BeautifulSoup(get_taxi_html(), 'html.parser')\n",
    "    yellow_pattern = r\"yellow_tripdata\"\n",
    "    # from Jan. 2009 to June 2015\n",
    "    year_pattern = r\"200\\d{1}\" # from Jan. 2009 to Dec. 2009\n",
    "    year_pattern2 = r\"201[01234]\" # from Jan. 2010 to Dec.2014\n",
    "    pattern2015 = r\"2015-0[123456]\" # from Jan. 2015 to June 2015\n",
    "    link_list = [a['href'] for a in soup.find_all('a')[30:-25]]\n",
    "    new_links = list()\n",
    "    for item in link_list:\n",
    "        # iterate through each year 2009 - 2015\n",
    "        if (re.search(yellow_pattern, item) != None): \n",
    "            if (re.search(year_pattern, item) != None):\n",
    "                new_links.append(item)\n",
    "            if (re.search(year_pattern2, item) != None):\n",
    "                new_links.append(item)\n",
    "            if (re.search(pattern2015, item) != None):\n",
    "                new_links.append(item)\n",
    "    return new_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    dataframe = pd.read_parquet(url,engine='pyarrow')\n",
    "    # Taking a sample of the taxi data:\n",
    "    return dataframe.sample(n = 23080, random_state=39)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884672ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miles_to_km(distance_miles):\n",
    "    distance_km = distance_miles /0.62137119\n",
    "    return distance_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df93585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Code in order to change the taxi zone to a latitude and longitude variable\n",
    "def get_lat_and_long_from_zone(zone):\n",
    "    import json\n",
    "    from shapely.geometry import Point\n",
    "    df = gpd.read_file('NYCTaxiZones.geojson')\n",
    "    taxi_zones = gpd.GeoDataFrame(df)\n",
    "    taxi_zones = taxi_zones.to_crs(4326)\n",
    "\n",
    "    taxi_zones['lon'] = taxi_zones.centroid.x  \n",
    "    taxi_zones['lat'] = taxi_zones.centroid.y\n",
    "    \n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        if zone == row[\"location_id\"]:\n",
    "            long = row['lon']\n",
    "            lat = row['lat']\n",
    "            return lat, long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a107f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zones_within_bbox():\n",
    "    import json\n",
    "    from shapely.geometry import Point\n",
    "    df = gpd.read_file('NYCTaxiZones.geojson')\n",
    "    taxi_zones = gpd.GeoDataFrame(df)\n",
    "    taxi_zones = taxi_zones.to_crs(4326)\n",
    "    taxi_zones['lon'] = taxi_zones.centroid.x  \n",
    "    taxi_zones['lat'] = taxi_zones.centroid.y\n",
    "    northlimit  = 40.908524\n",
    "    southlimit = 40.560445\n",
    "    eastlimit = -73.717047\n",
    "    westlimit = -74.242330\n",
    "    taxi_zones = taxi_zones[(taxi_zones[\"lon\"] <= eastlimit) & (taxi_zones[\"lon\"] >= westlimit)] \n",
    "    taxi_zones = taxi_zones[(taxi_zones[\"lat\"] <= northlimit) & (taxi_zones[\"lat\"]>= southlimit)]\n",
    "    zones_in_range =list(taxi_zones[\"location_id\"])\n",
    "    return zones_in_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    all_csv_urls = find_taxi_parquet_urls()\n",
    "    northlimit  = 40.908524\n",
    "    southlimit = 40.560445\n",
    "    eastlimit = -73.717047\n",
    "    westlimit = -74.242330\n",
    "    for csv_url in all_csv_urls:\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        # Making sure that the zone is in the [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "        if 'PULocationID' in dataframe.columns:\n",
    "            dataframe[dataframe['PULocationID'].isin(zones_within_bbox())]\n",
    "            dataframe[dataframe['DOLocationID'].isin(zones_within_bbox())]\n",
    "            # Changing the distance column of each data set from miles to kilometers\n",
    "            dataframe['trip_distance'] = dataframe.apply(lambda x: x[\"trip_distance\"]/0.62137119, axis = 1)\n",
    "            # Adding A Pickup Hour Column to the Data\n",
    "            dataframe['pickup_hour'] = dataframe['tpep_pickup_datetime'].apply(lambda x:x.hour)\n",
    "            # Removing Unnecessary Columns:\n",
    "            dataframe = dataframe.drop([\"VendorID\",\"payment_type\",\"airport_fee\",\"mta_tax\",\"store_and_fwd_flag\",\"tolls_amount\",\"congestion_surcharge\",\"RatecodeID\",\"extra\",\"improvement_surcharge\",\"fare_amount\",\"passenger_count\",\"total_amount\"],axis=1)\n",
    "\n",
    "            # Normalizing Column Names:\n",
    "            dataframe.rename(columns ={\"trip_distance\": \"distance\", \"tpep_pickup_datetime\": \"pickup_datetime\",\"tpep_dropoff_datetime\":\"dropoff_datetime\"}, inplace = True)\n",
    "        elif \"pickup_longitude\" in dataframe.columns:\n",
    "            # add_distance_column(dataframe)\n",
    "            dataframe = dataframe[(dataframe[\"pickup_longitude\"] <= eastlimit) & (dataframe[\"pickup_longitude\"] >= westlimit)] \n",
    "            dataframe = dataframe [(dataframe [\"pickup_latitude\"] <= northlimit) & (dataframe[\"pickup_latitude\"]>= southlimit)]\n",
    "            dataframe  = dataframe[(dataframe[\"dropoff_longitude\"] <= eastlimit) & (dataframe[\"dropoff_longitude\"] >= westlimit)] \n",
    "            dataframe = dataframe[(dataframe[\"dropoff_latitude\"] <= northlimit) & (dataframe[\"dropoff_latitude\"]>= southlimit)]\n",
    "\n",
    "            #Cleaning the date to make Datetime Object (already datetime object?)\n",
    "            dataframe[\"dropoff_datetime\"]  = dataframe[\"dropoff_datetime\"].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "            dataframe[\"pickup_datetime\"]  = dataframe[\"pickup_datetime\"].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "            dataframe['pickup_hour'] = dataframe['pickup_datetime'].apply(lambda x:x.hour)\n",
    "\n",
    "            dataframe = dataframe.drop([\"vendor_id\",\"payment_type\",\"mta_tax\",\"store_and_fwd_flag\",\"surcharge\",\"rate_code\",\"tolls_amount\",\"fare_amount\",\"passenger_count\",\"total_amount\"], axis = 1)\n",
    "            \n",
    "            # Changing the distance column of each data set from miles to kilometers\n",
    "            dataframe['trip_distance'] = dataframe.apply(lambda x: x[\"trip_distance\"]/0.62137119, axis = 1)\n",
    "            # Normalizing Column Names\n",
    "            dataframe.rename(columns ={\"trip_distance\": \"distance\"}, inplace = True)\n",
    "\n",
    "        elif \"Trip_Pickup_DateTime\" in dataframe.columns:\n",
    "\n",
    "            dataframe.rename(columns ={\"Start_Lon\":\"pickup_longitude\",\"Start_Lat\":\"pickup_latitude\",\"End_Lon\":\"dropoff_longitude\",\"End_Lat\":\"dropoff_latitude\"}, inplace = True)\n",
    "            dataframe = dataframe[(dataframe[\"pickup_longitude\"] <= eastlimit) & (dataframe[\"pickup_longitude\"] >= westlimit)] \n",
    "            dataframe = dataframe [(dataframe [\"pickup_latitude\"] <= northlimit) & (dataframe[\"pickup_latitude\"]>= southlimit)]\n",
    "\n",
    "            dataframe  = dataframe[(dataframe[\"dropoff_longitude\"] <= eastlimit) & (dataframe[\"dropoff_longitude\"] >= westlimit)] \n",
    "            dataframe = dataframe[(dataframe[\"dropoff_latitude\"] <= northlimit) & (dataframe[\"dropoff_latitude\"]>= southlimit)]\n",
    "\n",
    "            dataframe = dataframe.drop([\"vendor_name\",\"Total_Amt\",\"Tolls_Amt\",\"store_and_forward\",\"mta_tax\",\"surcharge\",\"Fare_Amt\",\"Payment_Type\",\"Rate_Code\",\"Passenger_Count\"], axis = 1)\n",
    "            dataframe[\"Trip_Pickup_DateTime\"]  = dataframe[\"Trip_Pickup_DateTime\"].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "            dataframe[\"Trip_Dropoff_DateTime\"]  = dataframe[\"Trip_Dropoff_DateTime\"].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "            dataframe['pickup_hour'] = dataframe[\"Trip_Pickup_DateTime\"].apply(lambda x:x.hour)\n",
    "            \n",
    "            dataframe['distance'] = dataframe.apply(lambda x: x[\"Trip_Distance\"]/0.62137119, axis = 1)\n",
    "\n",
    "            dataframe.rename(columns ={\"Tip_Amt\":\"tip_amount\",\"Trip_Pickup_DateTime\":\"pickup_datetime\",\"Trip_Dropoff_DateTime\":\"dropoff_datetime\"}, inplace = True)\n",
    "\n",
    "            dataframe = dataframe.drop([\"Trip_Distance\"],axis = 1)\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    #taxi_data = taxi_data.drop([\"Distance\"],axis = 1)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff321bec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "In this portion of the project, two functions load_and_clean_uber_data() and get_uber_data() are used.  The function load_and_clean_uber_data() reads in the uber data from a csv file and returns a dataframe.  The function get_uber_data() uses the previous function to read in the data, then uses the function add_distance_column previously defined in order to add the distance in kilometers of each trip taken by an uber in the dataset and returns the uber data as a dataframe.  In addition, some additional processing was done ot the data to convert the pickup_datetime into a datetime object and create a column for the day of the week the pickup occured on.  In addition any data that is outside of the bounding box (http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047) was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    return pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    # Making Pickup_datetime a datetime object\n",
    "    uber_dataframe[\"pickup_datetime\"]  = uber_dataframe[\"pickup_datetime\"].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S %Z'))\n",
    "    uber_dataframe[\"day_of_week\"]= uber_dataframe['pickup_datetime'].apply(lambda x: x.isoweekday())\n",
    "\n",
    "    # Removing any data outside of the [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    northlimit  = 40.908524\n",
    "    southlimit = 40.560445\n",
    "    eastlimit = -73.717047\n",
    "    westlimit = -74.242330\n",
    "    \n",
    "    uber_dataframe = uber_dataframe[(uber_dataframe[\"pickup_longitude\"] <= eastlimit) & (uber_dataframe[\"pickup_longitude\"] >= westlimit)] \n",
    "    uber_dataframe = uber_dataframe [(uber_dataframe [\"pickup_latitude\"] <= northlimit) & (uber_dataframe[\"pickup_latitude\"]>= southlimit)]\n",
    "\n",
    "    uber_dataframe  = uber_dataframe[(uber_dataframe[\"dropoff_longitude\"] <= eastlimit) & (uber_dataframe[\"dropoff_longitude\"] >= westlimit)] \n",
    "    uber_dataframe = uber_dataframe[(uber_dataframe[\"dropoff_latitude\"] <= northlimit) & (uber_dataframe[\"dropoff_latitude\"]>= southlimit)]\n",
    "\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "In processing the weather data, two different dataframes (hourly_weather_data, daily_weather_data) were created.  The daily dataframe has data for each day from January 2009 to June 2015 and any duplicate measurment for the days is dropped.  In the hourly data, the data is broken down to each hour of each day from January 2009 to June 2015 and it drops any duplicate measurments that are taken in a given hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    all_data = pd.read_csv(csv_file)\n",
    "\n",
    "    date = all_data['DATE']\n",
    "    import datetime\n",
    "    date = date.apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%dT%H:%M:%S'))\n",
    "    all_data['hours'] = date.apply(lambda x:x.hour)\n",
    "    all_data['newDATE'] = 0\n",
    "    for i in range(len(date)):\n",
    "        all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
    "    hourly_data = all_data\n",
    "    return hourly_data.drop_duplicates(subset=['hours', 'newDATE'],keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    all_data = pd.read_csv(csv_file)\n",
    "    date = all_data['DATE']\n",
    "    import datetime\n",
    "    date = date.apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%dT%H:%M:%S'))\n",
    "    all_data['days'] = date.apply(lambda x:x.day)\n",
    "    all_data['newDATE'] = 0\n",
    "    for i in range(len(date)):\n",
    "        all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
    "    daily_data = all_data\n",
    "    return daily_data.drop_duplicates(subset=['days', 'newDATE'],keep='last')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "\n",
    "    csv09_file = '2009_weather.csv'\n",
    "    csv10_file = '2010_weather.csv'\n",
    "    csv11_file =  '2011_weather.csv'\n",
    "    csv12_file = '2012_weather.csv'\n",
    "    csv13_file = '2013_weather.csv'\n",
    "    csv14_file = '2014_weather.csv'\n",
    "    csv15_file = '2015_weather.csv'\n",
    "    weather_csv09_14_files = [csv09_file, csv10_file, csv11_file, csv12_file, csv13_file, csv14_file]\n",
    "    \n",
    "    for csv_file in weather_csv09_14_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "    hourly_15_dataframe = clean_month_weather_data_hourly(csv15_file).iloc[:4344]\n",
    "    hourly_dataframes.append(hourly_15_dataframe)\n",
    "    daily_15_dataframe = clean_month_weather_data_daily(csv15_file).iloc[:181]\n",
    "    daily_dataframes.append(daily_15_dataframe)\n",
    "\n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes).reset_index(drop=True)\n",
    "    daily_data = pd.concat(daily_dataframes).reset_index(drop=True)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "Once all of the functions in order to process the data have been written each of those functions can be executed.  Executing each of these functions, provides four clean data sets, taxi_data, uber_data, hourly_weather_data, and daily_weather_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:3: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:3: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:3: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:3: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:3: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:3: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:3: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:3: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:3: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:3: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:3: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:3: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_44900/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37437cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f58a8a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10566313</th>\n",
       "      <td>2015-01-26 00:57:34</td>\n",
       "      <td>2015-01-26 01:06:47</td>\n",
       "      <td>3.862426</td>\n",
       "      <td>164.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100166</th>\n",
       "      <td>2015-01-17 21:43:25</td>\n",
       "      <td>2015-01-17 21:47:11</td>\n",
       "      <td>1.287475</td>\n",
       "      <td>234.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332144</th>\n",
       "      <td>2015-01-01 20:37:28</td>\n",
       "      <td>2015-01-01 20:48:13</td>\n",
       "      <td>2.156521</td>\n",
       "      <td>142.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841470</th>\n",
       "      <td>2015-01-03 10:43:15</td>\n",
       "      <td>2015-01-03 10:50:28</td>\n",
       "      <td>4.538350</td>\n",
       "      <td>229.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119963</th>\n",
       "      <td>2015-01-15 20:31:59</td>\n",
       "      <td>2015-01-15 20:36:17</td>\n",
       "      <td>1.303569</td>\n",
       "      <td>249.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8807639</th>\n",
       "      <td>2015-01-22 06:50:02</td>\n",
       "      <td>2015-01-22 06:55:14</td>\n",
       "      <td>1.931213</td>\n",
       "      <td>239.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193240</th>\n",
       "      <td>2015-01-09 09:03:50</td>\n",
       "      <td>2015-01-09 09:20:19</td>\n",
       "      <td>2.735885</td>\n",
       "      <td>24.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12323399</th>\n",
       "      <td>2015-01-31 07:50:47</td>\n",
       "      <td>2015-01-31 07:54:26</td>\n",
       "      <td>0.772485</td>\n",
       "      <td>164.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4697863</th>\n",
       "      <td>2015-01-12 16:42:51</td>\n",
       "      <td>2015-01-12 17:21:29</td>\n",
       "      <td>9.495130</td>\n",
       "      <td>186.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11030186</th>\n",
       "      <td>2015-01-28 10:37:55</td>\n",
       "      <td>2015-01-28 10:46:09</td>\n",
       "      <td>1.689811</td>\n",
       "      <td>237.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pickup_datetime    dropoff_datetime  distance  PULocationID  \\\n",
       "10566313 2015-01-26 00:57:34 2015-01-26 01:06:47  3.862426         164.0   \n",
       "7100166  2015-01-17 21:43:25 2015-01-17 21:47:11  1.287475         234.0   \n",
       "332144   2015-01-01 20:37:28 2015-01-01 20:48:13  2.156521         142.0   \n",
       "841470   2015-01-03 10:43:15 2015-01-03 10:50:28  4.538350         229.0   \n",
       "6119963  2015-01-15 20:31:59 2015-01-15 20:36:17  1.303569         249.0   \n",
       "8807639  2015-01-22 06:50:02 2015-01-22 06:55:14  1.931213         239.0   \n",
       "3193240  2015-01-09 09:03:50 2015-01-09 09:20:19  2.735885          24.0   \n",
       "12323399 2015-01-31 07:50:47 2015-01-31 07:54:26  0.772485         164.0   \n",
       "4697863  2015-01-12 16:42:51 2015-01-12 17:21:29  9.495130         186.0   \n",
       "11030186 2015-01-28 10:37:55 2015-01-28 10:46:09  1.689811         237.0   \n",
       "\n",
       "          DOLocationID  tip_amount  pickup_hour  pickup_longitude  \\\n",
       "10566313          79.0        1.50            0               NaN   \n",
       "7100166           90.0        1.45           21               NaN   \n",
       "332144           161.0        0.00           20               NaN   \n",
       "841470           148.0        1.90           10               NaN   \n",
       "6119963          114.0        1.38           20               NaN   \n",
       "8807639          142.0        0.00            6               NaN   \n",
       "3193240          143.0        0.00            9               NaN   \n",
       "12323399         170.0        1.00            7               NaN   \n",
       "4697863            7.0        0.00           16               NaN   \n",
       "11030186         162.0        0.00           10               NaN   \n",
       "\n",
       "          pickup_latitude  dropoff_longitude  dropoff_latitude  \n",
       "10566313              NaN                NaN               NaN  \n",
       "7100166               NaN                NaN               NaN  \n",
       "332144                NaN                NaN               NaN  \n",
       "841470                NaN                NaN               NaN  \n",
       "6119963               NaN                NaN               NaN  \n",
       "8807639               NaN                NaN               NaN  \n",
       "3193240               NaN                NaN               NaN  \n",
       "12323399              NaN                NaN               NaN  \n",
       "4697863               NaN                NaN               NaN  \n",
       "11030186              NaN                NaN               NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "Once the data was read and cleaned it is stored in a SQL database. In this portion, four tables (hourly_weather, daily_weather, taxi_trips, and uber_trips) were created to store the different dataframes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)\n",
    "# First, using SQLAlchemy, create a SQLite database with which you’ll load in your preprocessed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    LATITUDE FLOAT,\n",
    "    LONGITUDE FLOAT,\n",
    "    NAME TEXT,\n",
    "    HourlyPrecipitation STRING,\n",
    "    HourlyWindDirection STRING,\n",
    "    HourlyWindGustSpeed STRING,\n",
    "    HourlyWindSpeed\tSTRING,\n",
    "    hours INTEGER,\n",
    "    newDATE STRING\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    LATITUDE FLOAT,\n",
    "    LONGITUDE FLOAT,\n",
    "    NAME TEXT,\n",
    "    DailyAverageWindSpeed STRING,\n",
    "    DailyPeakWindDirection STRING,\n",
    "    DailyPeakWindSpeed STRING,\n",
    "    DailyPrecipitation STRING,\n",
    "    DailySustainedWindDirection STRING,\n",
    "    DailySustainedWindSpeed STRING,\n",
    "    days INTEGER,\n",
    "    newDATE STRING\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\t\n",
    "    pickup_datetime\tTIMESTAMP,\n",
    "    dropoff_datetime DATE,\n",
    "    distance FLOAT,\n",
    "    PULocationID FLOAT,\n",
    "    DOLocationID FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    pickup_hour FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    fare_amount\tFLOAT,\n",
    "    pickup_datetime TIMESTAMP,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    passenger_count INTEGER,\n",
    "    Distance FLOAT,\n",
    "    day_of_week INTEGER\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    engine.connect().execute(\n",
    "        HOURLY_WEATHER_SCHEMA\n",
    "    )\n",
    "    engine.connect().execute(\n",
    "        DAILY_WEATHER_SCHEMA\n",
    "    )\n",
    "    engine.connect().execute(\n",
    "        TAXI_TRIPS_SCHEMA\n",
    "    )\n",
    "    engine.connect().execute(\n",
    "        UBER_TRIPS_SCHEMA\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f527aa4",
   "metadata": {},
   "source": [
    "pd.read_sql_query - read data from querying a SQL table\n",
    "\n",
    "pd.read_sql_table - read entire SQL table\n",
    "\n",
    "df.to_sql - add data from the dataframe to a SQL table\n",
    "\n",
    "pd.to_numeric - Convert argument to a numeric type\n",
    "\n",
    "pd.concat - Concatenate pandas objects along a particular axis with optional set logic along the other axes\n",
    "\n",
    "pd.merge - Merge DataFrame or named Series objects with a database-style join\n",
    "\n",
    "pd.merge_asof - Perform a merge by key distance. This is similar to a left-join except that we match on the nearest key rather than equal keys. Both DataFrames must be sorted by the key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "Once the data is cleaned and the SQL tables are set up, the data that was read in to the file early can be added to the database, which is done during this step for each of the four tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for item_0, item_1 in table_to_df_dict.items():\n",
    "        # add data from the dataframe to a SQL table\n",
    "        item_1.to_sql(item_0, engine, index_label=\"id\", if_exists=\"append\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e6d3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data to pick useful columns and ignore others\n",
    "taxi_data_clean = taxi_data.set_index([pd.Series(np.arange(len(taxi_data['distance'])))])  \n",
    "uber_data_clean = uber_data.iloc[:,2:11]\n",
    "hourly_data = hourly_weather_data.iloc[:, [1,2,3,5,11,21,22,23,-2,-1]]\n",
    "daily_data = daily_weather_data.iloc[:, [1,2,3,5,32,38,39,40,43,44,-2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data_clean,\n",
    "    \"uber_trips\": uber_data_clean,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6ce43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sqlite3 project.db < schema.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "976486aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_sql_table(\"daily_weather\", con=DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d98db556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_sql_table(\"uber_trips\", con=DATABASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287644ea",
   "metadata": {},
   "source": [
    "In this section, six queries are done to the SQL tables in order to determine the following information about taxi and uber trips:\n",
    "* For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7371b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips\n",
    "# You may wish to use SQLAlchemy within the notebook to help craft these queries and query files. \n",
    "# You can also use pandas to help check the validity of your queries.\n",
    "# You may want to familiarize yourself with the WITH <name> AS and WITH RECURSIVE <name> AS expressions in SQL. \n",
    "# This is a good resource with a lot of examples to help get familiar.\n",
    "# There are quite a few examples using UNION ALL - this will help \"flatten\" tables that have common columns \n",
    "# (e.g. fare amount, pickup dates, etc) into a singular shared column (for example, taxi fares and uber fares are in one column).\n",
    "# Look at the example query that starts with WITH RECURSIVE dates(x) AS for help with the 6th query.\n",
    "# You may also want to familiarize yourself with the COALESCE expression in SQL. This is a decent tutorial to look through.\n",
    "# This Stack Overflow post will be helpful when crafting queries for figuring out percentiles/quantiles.\n",
    "# This Stack Overflow post will be helpful for grouping results by timeframe, e.g. by hour, day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as o:\n",
    "        o.write(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576ef10",
   "metadata": {},
   "source": [
    "### For the same time frame, what day of the week was the most popular to take an uber? \n",
    "In this portion, in order to determine the most popular day of the week for uber trips, the number of times a certain day of week appears in the dataset is counted and then ordered by days of that count. \n",
    "\n",
    "\n",
    "From the output, we find Friday is the most popular day to take an uber among the week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e142954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def uber_popular_day_of_week(df = uber_data):\n",
    "     \n",
    "#      day_of_week_group = df.groupby('day_of_week').size().sort_values(ascending=False)\n",
    "#      return day_of_week_group\n",
    "\n",
    "# uber_popular_day_of_week()\n",
    "Uber_day_most_pop = \"\"\"\n",
    "    SELECT day_of_week FROM uber_trips GROUP BY day_of_week ORDER BY COUNT(day_of_week) DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51fb9fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5,), (6,), (4,), (3,), (2,), (7,), (1,)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(Uber_day_most_pop).fetchall()\n",
    "# Friday is the most popular day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08633f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(6,)\n",
      "(4,)\n",
      "(3,)\n",
      "(2,)\n",
      "(7,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "result = engine.execute(Uber_day_most_pop).fetchall()\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2436db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(Uber_day_most_pop, \"Uber_most_pop_day.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b5b91",
   "metadata": {},
   "source": [
    "### Most Popular Hour Taxis Query\n",
    "\n",
    "_**TODO:**  In this section, in order to determine the post popular hour for a taxi pickup to occur at, the number of times a pickup hour occurs in the database is counted, then it is ordered by that count.\n",
    "\n",
    "We find out the most popular hour of pickup is 7p.m.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "228ef488",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_hour_most_pop = \"\"\"\n",
    "    SELECT pickup_hour, COUNT(pickup_hour) FROM taxi_trips GROUP BY pickup_hour ORDER BY COUNT(pickup_hour) DESC\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6652192d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19.0, 111676),\n",
       " (18.0, 107562),\n",
       " (20.0, 105567),\n",
       " (21.0, 102491),\n",
       " (22.0, 99690),\n",
       " (14.0, 90793),\n",
       " (23.0, 88970),\n",
       " (17.0, 88767),\n",
       " (12.0, 87899),\n",
       " (13.0, 87399),\n",
       " (15.0, 85954),\n",
       " (9.0, 83415),\n",
       " (11.0, 82853),\n",
       " (8.0, 81141),\n",
       " (10.0, 79896),\n",
       " (16.0, 74176),\n",
       " (0.0, 71349),\n",
       " (7.0, 64168),\n",
       " (1.0, 52371),\n",
       " (2.0, 39011),\n",
       " (6.0, 36858),\n",
       " (3.0, 28697),\n",
       " (4.0, 21088),\n",
       " (5.0, 17558)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(taxi_hour_most_pop).fetchall()\n",
    "# The most popular hour for a tzxi pickup to occur is at 19(same as 7p.m.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7670ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(taxi_hour_most_pop, \"Taxi_hour_most_pop.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe4210",
   "metadata": {},
   "source": [
    "### What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here.\n",
    "\n",
    "\n",
    "Those are what we found: (top 10 days, number of hired rides, average distance)\n",
    "* ('2009-01-24', 954, 4.144043945487254), \n",
    "* ('2009-02-06', 950, 3.954700317060508),\n",
    "* ('2009-02-13', 942, 4.41544541717232),\n",
    "* ('2009-02-20', 937, 4.269175717911793),\n",
    "* ('2009-12-18', 932, 4.236252742463521),\n",
    "* ('2009-11-07', 928, 4.112689012049695),\n",
    "* ('2009-02-14', 922, 4.161969567089876),\n",
    "* ('2009-06-05', 910, 4.084019903935855),\n",
    "* ('2009-02-05', 909, 3.9356516513985897),\n",
    "* ('2009-09-10', 909, 4.2768361215379675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad64e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_days_2009 = \"\"\"\n",
    "SELECT DATE(pickup_datetime), COUNT(DATE(pickup_datetime)), AVG(distance)\n",
    "FROM taxi_trips \n",
    "WHERE pickup_datetime LIKE '2009-%-%'\n",
    "GROUP BY DATE(pickup_datetime)\n",
    "UNION\n",
    "SELECT DATE(pickup_datetime), COUNT(DATE(pickup_datetime)), AVG(Distance)\n",
    "FROM uber_trips \n",
    "WHERE pickup_datetime LIKE '2009-%-%'\n",
    "GROUP BY DATE(pickup_datetime) \n",
    "ORDER BY COUNT(DATE(pickup_datetime)) DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0047991a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2009-01-24', 954, 4.144043945487254),\n",
       " ('2009-02-06', 950, 3.954700317060508),\n",
       " ('2009-02-13', 942, 4.41544541717232),\n",
       " ('2009-02-20', 937, 4.269175717911793),\n",
       " ('2009-12-18', 932, 4.236252742463521),\n",
       " ('2009-11-07', 928, 4.112689012049695),\n",
       " ('2009-02-14', 922, 4.161969567089876),\n",
       " ('2009-06-05', 910, 4.084019903935855),\n",
       " ('2009-02-05', 909, 3.9356516513985897),\n",
       " ('2009-09-10', 909, 4.2768361215379675)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(top_10_days_2009).fetchall()\n",
    "# TOP 10 days, no. of rides, average distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ec07b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(top_10_days_2009, \"top_10_days_2009.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62d2ff1d",
   "metadata": {},
   "source": [
    "### Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "\n",
    "\n",
    "Those are what we found: (top 10 days, avg. wind speed, number of hired trips)\n",
    "* ('2014-03-13', 14.1, 934),\n",
    "* ('2014-01-07', 13.1, 869),\n",
    "* ('2014-02-13', 12.6, 652),\n",
    "* ('2014-01-02', 12.2, 652),\n",
    "* ('2014-03-26', 11.9, 811),\n",
    "* ('2014-12-07', 11.8, 904),\n",
    "* ('2014-12-08', 11.5, 805),\n",
    "* ('2014-11-02', 10.8, 847),\n",
    "* ('2014-03-29', 10.8, 994),\n",
    "* ('2014-02-14', 10.4, 854)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39cd612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather data below\n",
    "windiest_days= \"\"\"\n",
    "WITH taxi_trip(select_day, count) AS (\n",
    "    SELECT DATE(pickup_datetime) AS select_day, COUNT(DATE(pickup_datetime)) AS count\n",
    "    FROM taxi_trips\n",
    "    WHERE select_day LIKE '2014-%-%'\n",
    "    GROUP BY select_day\n",
    "    ORDER BY count DESC\n",
    "    ),\n",
    "    uber_trip(select_day, count) AS (\n",
    "    SELECT DATE(pickup_datetime) AS select_day, COUNT(DATE(pickup_datetime)) AS count\n",
    "    FROM uber_trips\n",
    "    WHERE select_day LIKE '2014-%-%'\n",
    "    GROUP BY select_day\n",
    "    ORDER BY count DESC\n",
    "    ),\n",
    "    daily_w(select_day, daw) AS (\n",
    "    SELECT DATE(DATE) AS select_day, DailyAverageWindSpeed AS daw\n",
    "    FROM daily_weather\n",
    "    WHERE select_day LIKE '2014-%-%'\n",
    "    GROUP BY select_day\n",
    "    ORDER BY daw DESC\n",
    "    LIMIT 10\n",
    "    )\n",
    "    SELECT daily_w.select_day, daily_w.daw, taxi_trip.count+uber_trip.count\n",
    "    FROM daily_w\n",
    "    LEFT JOIN taxi_trip ON daily_w.select_day = taxi_trip.select_day\n",
    "    LEFT JOIN uber_trip ON daily_w.select_day = uber_trip.select_day;\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba588b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014-03-13', 14.1, 934),\n",
       " ('2014-01-07', 13.1, 869),\n",
       " ('2014-02-13', 12.6, 652),\n",
       " ('2014-01-02', 12.2, 652),\n",
       " ('2014-03-26', 11.9, 811),\n",
       " ('2014-12-07', 11.8, 904),\n",
       " ('2014-12-08', 11.5, 805),\n",
       " ('2014-11-02', 10.8, 847),\n",
       " ('2014-03-29', 10.8, 994),\n",
       " ('2014-02-14', 10.4, 854)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(windiest_days).fetchall()\n",
    "# (top 10 days, avg_wind_speed, hired trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "810602a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(windiest_days, \"windiest_days.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233493b9",
   "metadata": {},
   "source": [
    "### What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "\n",
    "We found 15.771571256787752 is the 95% percentile of distance traveled for all hired trips during July 2013.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2e5ac506",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_95th_2013 =\"\"\"\n",
    "SELECT distance AS distance, DATE(pickup_datetime) AS date\n",
    "FROM taxi_trips\n",
    "WHERE date LIKE '2013-07-%'\n",
    "UNION ALL\n",
    "SELECT Distance AS distance, DATE(pickup_datetime) AS date\n",
    "FROM uber_trips\n",
    "WHERE date LIKE '2013-07-%'\n",
    "ORDER BY distance\n",
    "LIMIT 1\n",
    "OFFSET (\n",
    "    WITH t(count) AS\n",
    "    (SELECT COUNT(distance) AS count\n",
    "    FROM taxi_trips\n",
    "    WHERE DATE(pickup_datetime) LIKE '2013-07-%'),\n",
    "    u(count) AS\n",
    "    (SELECT COUNT(Distance) AS count\n",
    "    FROM uber_trips\n",
    "    WHERE DATE(pickup_datetime) LIKE '2013-07-%')\n",
    "    SELECT t.count+u.count\n",
    "    FROM t\n",
    "    JOIN u)*95/100-1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "315fdf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15.771571256787752, '2013-07-29')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(distance_95th_2013).fetchall()\n",
    "# 95% percentile, the corresponding date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "875e09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(distance_95th_2013, \"95th_distance_2013.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0963a0f",
   "metadata": {},
   "source": [
    "### During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it and the week after, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?\n",
    "\n",
    "Those are what we found: (hour, COUNT(trips), precipitation, Hourlywindspeed)\n",
    "* (0, 415, 0.0013333333333333333, 6.0),\n",
    "* (1, 350, 0.0006666666666666666, 4.9375),\n",
    "* (2, 250, 0.002142857142857143, 5.25),\n",
    "* (3, 177, 0.0026666666666666666, 6.8),\n",
    "* (4, 142, 0.0, 6.2),\n",
    "* (5, 128, 0.0006666666666666666, 5.0625),\n",
    "* (6, 245, 0.002, 6.5625),\n",
    "* (7, 421, 0.0013333333333333333, 5.928571428571429),\n",
    "* (8, 497, 0.0033333333333333335, 6.8),\n",
    "* (9, 499, 0.006, 6.625),\n",
    "* (10, 502, 0.002, 5.666666666666667),\n",
    "* (11, 607, 0.0, 6.533333333333333),\n",
    "* (12, 601, 0.00375, 7.375),\n",
    "* (13, 545, 0.002857142857142857, 7.615384615384615),\n",
    "* (14, 580, 0.0013333333333333333, 8.384615384615385),\n",
    "* (15, 522, 0.007333333333333333, 7.769230769230769),\n",
    "* (16, 500, 0.006666666666666666, 7.6),\n",
    "* (17, 586, 0.004666666666666667, 7.0),\n",
    "* (18, 661, 0.0013333333333333333, 7.133333333333334),\n",
    "* (19, 706, 0.000625, 6.625),\n",
    "* (20, 705, 0.00125, 6.666666666666667),\n",
    "* (21, 656, 0.0, 6.9375),\n",
    "* (22, 641, 0.001875, 5.6875),\n",
    "* (23, 556, None, None) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "607aece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy = \"\"\" \n",
    "WITH taxi(hour, count) AS (\n",
    "    SELECT CAST(pickup_hour AS int) AS hour, COUNT(pickup_hour) AS count\n",
    "    FROM taxi_trips\n",
    "    WHERE DATE(pickup_datetime) BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "    GROUP BY hour),\n",
    "    uber(hour, count) AS (\n",
    "    SELECT CAST(strftime('%H', pickup_datetime) AS int) AS hour, COUNT(strftime('%H', pickup_datetime)) AS count\n",
    "    FROM uber_trips\n",
    "    WHERE DATE(pickup_datetime) BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "    GROUP BY hour),\n",
    "    weather(hour, precip, wspeed) AS (\n",
    "    SELECT CAST(hours AS int) AS hour, AVG(HourlyPrecipitation) AS precip, AVG(HourlyWindSpeed) AS wspeed\n",
    "    FROM hourly_weather\n",
    "    WHERE DATE(DATE) BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "    GROUP BY hour)\n",
    "    SELECT weather.hour, taxi.count+uber.count, weather.precip, weather.wspeed\n",
    "    FROM weather\n",
    "    LEFT JOIN taxi ON taxi.hour = weather.hour\n",
    "    LEFT JOIN uber ON uber.hour = weather.hour;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8db4303a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 415, 0.0013333333333333333, 6.0),\n",
       " (1, 350, 0.0006666666666666666, 4.9375),\n",
       " (2, 250, 0.002142857142857143, 5.25),\n",
       " (3, 177, 0.0026666666666666666, 6.8),\n",
       " (4, 142, 0.0, 6.2),\n",
       " (5, 128, 0.0006666666666666666, 5.0625),\n",
       " (6, 245, 0.002, 6.5625),\n",
       " (7, 421, 0.0013333333333333333, 5.928571428571429),\n",
       " (8, 497, 0.0033333333333333335, 6.8),\n",
       " (9, 499, 0.006, 6.625),\n",
       " (10, 502, 0.002, 5.666666666666667),\n",
       " (11, 607, 0.0, 6.533333333333333),\n",
       " (12, 601, 0.00375, 7.375),\n",
       " (13, 545, 0.002857142857142857, 7.615384615384615),\n",
       " (14, 580, 0.0013333333333333333, 8.384615384615385),\n",
       " (15, 522, 0.007333333333333333, 7.769230769230769),\n",
       " (16, 500, 0.006666666666666666, 7.6),\n",
       " (17, 586, 0.004666666666666667, 7.0),\n",
       " (18, 661, 0.0013333333333333333, 7.133333333333334),\n",
       " (19, 706, 0.000625, 6.625),\n",
       " (20, 705, 0.00125, 6.666666666666667),\n",
       " (21, 656, 0.0, 6.9375),\n",
       " (22, 641, 0.001875, 5.6875),\n",
       " (23, 556, None, None)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(sandy).fetchall()\n",
    "# hour, COUNT(trips), precipitation, Hourlywindspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d97d083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(sandy, \"Sandy.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51149848",
   "metadata": {},
   "source": [
    "### Visualization: For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? \n",
    "The plot below uses all the yellow taxi data to determine the most popular hour of the day for pickups.  Pickups peak around the 19th hour of the day, as the analysis has a zero hour this would be around 8 PM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016671a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_taxi_hour(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = dataframe['pickup_hour']  # use the dataframe to pull out values needed to plot\n",
    "    counts = dataframe['COUNT(pickup_hour)']\n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    # axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # axes.plot(values, counts)\n",
    "    plt.bar(values, counts)\n",
    "    plt.ylabel(\"Number of Taxi Trips\")\n",
    "    plt.xlabel(\"Pickup Hour\")\n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Most Popular Hour of the Day to take a Yellow Taxi January 2009 to June 2015\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a50423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_data_for_visual_taxi_hour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc27581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_taxi_hour():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    # Read the sql file\n",
    "    query = open('Taxi_hour_most_pop.sql', 'r')\n",
    "    # connection == the connection to your database, in your case prob_db\n",
    "    data = pd.read_sql_query(query.read(),DATABASE_URL)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_hour_most_pop = get_data_for_visual_taxi_hour()\n",
    "plot_visual_taxi_hour(taxi_hour_most_pop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9877d",
   "metadata": {},
   "source": [
    "### Visualization: Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR. Compares what day of the week was most popular for drop offs for each airport.\n",
    "\n",
    "The coordinates used for each respective airport:\n",
    "* EWR (Newark): -74.195995,40.664103,-74.148445,40.713045\n",
    "* JFK: -73.832496,40.618362,-73.744262,40.669421\n",
    "* LGA (LaGuardia): -73.892010,40.764638,-73.852357,40.787711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airport(lon,lat):\n",
    "    #MY CODE STARTS HERE\n",
    "    # long is x lat is y \n",
    "    southwestEWR = [-74.195995,40.664103]\n",
    "    southeastEWR = [-74.148445,40.664103]\n",
    "    northeastEWR = [-74.148445,40.713045]\n",
    "    northwestEWR = [-74.195995,40.713045]\n",
    "    EWR = (('EWR'),[southwestEWR,southeastEWR,northeastEWR,northwestEWR,southwestEWR])\n",
    "    southwestJFK = [-73.832496,40.618362]\n",
    "    southeastJFK = [-73.744262,40.618362]\n",
    "    northeastJFK = [-73.744262,40.669421]\n",
    "    northwestJFK = [-73.832496,40.669421]\n",
    "    JFK = (('JFK'),[southwestJFK,southeastJFK,northeastJFK,northwestJFK,southwestJFK])\n",
    "    southwestLGA = [-73.892010,40.764638]\n",
    "    southeastLGA = [-73.852357,40.764638]\n",
    "    northeastLGA = [-73.852357,40.787711]\n",
    "    northwestLGA = [-73.892010,40.787711]\n",
    "    LGA = (('LGA'),[southwestLGA,southeastLGA,northeastLGA,northwestLGA,southwestLGA])\n",
    "    airports = [EWR,JFK,LGA]\n",
    "    for item in airports:\n",
    "        #print(item)\n",
    "        if (item[1][0][0] <= lon <= item[1][1][0]) & (item[1][0][1] <= lat <= item[1][2][1]):\n",
    "            zone = item[0]\n",
    "            # want to return airport name \n",
    "            return zone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_airports(dataframe):\n",
    "    month_df= dataframe.groupby([\"Month\",\"dropoff_zone\"])\n",
    "    month_df.size().unstack().plot(kind='bar',figsize = (15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_airports():\n",
    "    query = \"\"\"SELECT DATE(pickup_datetime), dropoff_longitude, dropoff_latitude FROM uber_trips\n",
    "    UNION\n",
    "    SELECT DATE(pickup_datetime), dropoff_longitude, dropoff_latitude FROM taxi_trips \n",
    "    WHERE pickup_longitude IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df[\"DATE(pickup_datetime)\"] = df[\"DATE(pickup_datetime)\"].astype('datetime64[ns]')\n",
    "    df['Month'] = df[\"DATE(pickup_datetime)\"].apply(lambda x:x.weekday())\n",
    "\n",
    "\n",
    "    # Adding Airport\n",
    "    df['dropoff_zone'] = df.apply(lambda x: get_airport(x[\"dropoff_longitude\"], x[\"dropoff_latitude\"]),axis =1)\n",
    "    df = df.dropna()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd695dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_dataframe = get_data_for_airports()\n",
    "plot_visual_airports(airports_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595367c",
   "metadata": {},
   "source": [
    "### Visualization:  A heatmap of all hired trips over a map of the area\n",
    "\n",
    "A heatmap is generated from all of the hired trips over NYC, it is saved as an html file in which the visualization can be interacted with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf53e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_heatmap(dataframe):\n",
    "\n",
    "    map = KeplerGl(height=600, width=800)\n",
    "    gdf = gpd.GeoDataFrame(dataframe, geometry=gpd.points_from_xy(dataframe.pickup_longitude, dataframe.pickup_latitude))\n",
    "\n",
    "    map.add_data(data=gdf, name=\"Hired Trips\")\n",
    "    map.save_to_html(file_name='hired_trips_heatmap.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02db484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_heatmap():\n",
    "    query = \"\"\"SELECT pickup_longitude, pickup_latitude FROM uber_trips\n",
    "    UNION\n",
    "    SELECT pickup_longitude, pickup_latitude FROM taxi_trips \n",
    "    WHERE pickup_longitude IS NOT NULL\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_dataframe = get_data_for_heatmap()\n",
    "plot_visual_heatmap(heatmap_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19657f",
   "metadata": {},
   "source": [
    "### Visualization: The average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "\n",
    "Below, the months with a average distance, without considering major outliers in the distance traveled for months regardless of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14353e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_distance_month(dataframe):\n",
    "    avg_distance= dataframe.groupby('Month').mean()\n",
    "    std_distance = dataframe.groupby('Month').std()\n",
    "    count_distance = dataframe.groupby('Month').count()\n",
    "    stats = dataframe.groupby(['Month'])['Distance'].agg(['mean', 'count', 'std'])\n",
    "    ci95_hi = []\n",
    "    ci95_lo = []\n",
    "    error_amt = []\n",
    "    \n",
    "    for i in stats.index:\n",
    "        m, c, s = stats.loc[i]\n",
    "        ci95_hi.append(m + 1.645*s/math.sqrt(c))\n",
    "        ci95_lo.append(m - 1.645*s/math.sqrt(c))\n",
    "        error_amt.append(1.645*s/math.sqrt(c))\n",
    "\n",
    "    stats['ci95_hi'] = ci95_hi\n",
    "    stats['ci95_lo'] = ci95_lo\n",
    "    stats[\"error_amt\"] = error_amt\n",
    "\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    plt.bar(avg_distance.index, avg_distance[\"Distance\"],alpha = 0.5)\n",
    "    \n",
    "    # plt.errorbar(avg_distance.index, avg_distance[\"Distance\"],yerr=std_distance[\"Distance\"]*1.645)\n",
    "\n",
    "    plt.errorbar(avg_distance.index, avg_distance[\"Distance\"],yerr=stats[\"error_amt\"])\n",
    "    # /math.sqrt(count_distance[\"Distance\"])\n",
    "    axes.set_title(\"Average Distance Traveled Each Month with 90 Percent Confidence Interval\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27350918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_avg_trip_distance_per_month():\n",
    "    query = \"\"\"SELECT DATE(pickup_datetime), Distance FROM uber_trips\n",
    "    UNION\n",
    "    SELECT DATE(pickup_datetime), distance FROM taxi_trips \n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df= df[df.Distance<300]\n",
    "    df[\"DATE(pickup_datetime)\"] = df[\"DATE(pickup_datetime)\"].astype('datetime64[ns]')\n",
    "    df['Month'] = df[\"DATE(pickup_datetime)\"].apply(lambda x:x.month)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611dd4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_month = get_data_for_avg_trip_distance_per_month()\n",
    "plot_visual_distance_month(distance_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528da4c7",
   "metadata": {},
   "source": [
    "### Visualization:  A scatter plot that compares tip amount versus precipitation amount\n",
    "Comparing Tip amount for taxis to the amount of precipitation at the time of the ride. In this visualization, one tip amount was exclueded as an outlier. This amount was over $800."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3012e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_tip_vs_preciptitation(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    dataframe =dataframe[dataframe.tip<800]\n",
    "    plt.scatter(dataframe.precip,dataframe.tip)\n",
    "    plt.xlim([0.01126, 0.01127])\n",
    "    axes.set_title(\"Preciptiation During Each Hour Compared with Tip amount Of Those Rides\")\n",
    "    plt.ylabel(\"Tip Amount (Dollars)\")\n",
    "    plt.xlabel(\"Precipitation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b67c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_tip_vs_preciptiation():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    query = \"\"\"WITH taxi(hour, tip) AS (\n",
    "    SELECT CAST(pickup_hour AS int) AS hour, tip_amount AS tip\n",
    "    FROM taxi_trips),\n",
    "    weather(hour, precip) AS (\n",
    "    SELECT CAST(hours AS int) AS hour, AVG(HourlyPrecipitation) AS precip\n",
    "    FROM hourly_weather)\n",
    "    SELECT taxi.tip, weather.hour, weather.precip\n",
    "    FROM weather\n",
    "    LEFT JOIN taxi ON taxi.hour = weather.hour\n",
    "    \"\"\"\n",
    "    data = pd.read_sql_query(query, engine)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_vs_preciptitation_dataframe = get_data_for_visual_tip_vs_preciptiation()\n",
    "plot_visual_tip_vs_preciptitation(tip_vs_preciptitation_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f240076",
   "metadata": {},
   "source": [
    "### Visualization:  A scatter plot that compares tip amount versus distance\n",
    "Below is a plot that compares the distance traveled to the tip amount recieved by taxi drivers in the NYC area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c011ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_tip_distance(dataframe):\n",
    "\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    plt.scatter(dataframe.distance,dataframe.tip_amount)\n",
    "    axes.set_title(\"Trip Distance Compared with Tip amount\")\n",
    "    plt.ylabel(\"Tip Amount (Dollars)\")\n",
    "    plt.xlabel(\"Distance (Miles)\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_distance_tip():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    query = \"SELECT tip_amount, distance from taxi_trips\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df = df[df.distance<400]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103741b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_tip_dataframe = get_data_for_visual_distance_tip()\n",
    "plot_visual_tip_distance(distance_tip_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fa68bb73451efc61706d715b46efd745ccab7b86462809ef43c0a1824a5bb9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
