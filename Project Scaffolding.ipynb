{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "# !pip install matplotlib\n",
    "# !pip install requests\n",
    "# !pip install bs4\n",
    "# !pip install sqlalchemy\n",
    "# !pip install pandas\n",
    "# !pip install geojsonio --upgrade\n",
    "# !pip install geopandas\n",
    "import math\n",
    "from math import *\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "import datetime\n",
    "import geojsonio\n",
    "import numpy as np\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any general notebook setup, like log formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "# weather csv data file\n",
    "# 2009_weather to 2015_weather (just pick the first 6 months)\n",
    "csv__= '/Users/morax/Documents/哥大/IEORE4501/IEOR4501 HW/IEOR4501 Project/'\n",
    "csv09_file = csv__ + '2009_weather.csv'\n",
    "csv10_file = csv__ + '2010_weather.csv'\n",
    "csv11_file = csv__ + '2011_weather.csv'\n",
    "csv12_file = csv__ + '2012_weather.csv'\n",
    "csv13_file = csv__ + '2013_weather.csv'\n",
    "csv14_file = csv__ + '2014_weather.csv'\n",
    "csv15_file = csv__ + '2015_weather.csv'\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [x] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [x] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what you’re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "In this section in order to calculate the distance between two points in the uber data, we must use the longitude and latitude of the pickup and drop off locations.  Therefore, by using math module in order to calculate the distance between these two coordinates is given in the function calculate_distance().  It is important to note that, there are more accurate ways to calculate the distance based upon longitude and latitude that do not use just the math module.  In addtion, in the taxi data sets, the distance is already calculated, however, it is given in miles, so the miles_to_km() function converts the miles to kilometers so that a direct comparison between taxis and ubers can be made.  It should also be noted that the taxi distance is a distance driving on city streets whereas the uber distance is just a birdseye view distance, therefore, the distances in the taxi data set are likely to be slightly longer.  The function, add_distance_column() can be used in order to add the calculated distance to the uber data set.  This function uses the pickup and drop off longitudes and latitudes in order to build the additional column row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance1(from_coord, to_coord):\n",
    "    # Longitude is x, Latitude is y, \n",
    "    # Longitude x\n",
    "    long = (to_coord[0]-from_coord[0])*40000*math.cos((to_coord[1]+from_coord[1])*math.pi/360)/360\n",
    "    # Latitude y\n",
    "    lat = (to_coord[1]-from_coord[1])*40000/360\n",
    "    # so the distance is just the side z followed by x^2+y^2=z^2\n",
    "    distance = sqrt(long*long+lat*lat)\n",
    "    return distance\n",
    "\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "def calculate_distance2(from_coord, to_coord):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    # Longitude is x, Latitude is y, \n",
    "    # math.radians() converts a degree value into radians. \n",
    "    lon1 = radians(from_coord[0])\n",
    "    lat1 = radians(from_coord[1])\n",
    "    lon2 = radians(to_coord[0])\n",
    "    lat2 = radians(to_coord[1])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# well sometime calculate_distance1 is better than calculate_distance2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc07e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miles_to_km(distance_miles):\n",
    "    distance_km = distance_miles /0.62137119\n",
    "    return distance_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fix the variable names called in lambda function\n",
    "def add_distance_column(dataframe): \n",
    "    dataframe['Distance'] = dataframe.apply(lambda x: calculate_distance((x['pickup_longitude'], x[\"pickup_latitude\"]), (x['dropoff_longitude'], x['dropoff_latitude'])),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "In this section, the taxi data set is being processed, the first step in processing the taxi data is first finding the links, then once those links are found they must be read in, and processed by combining each dataset for each month and year into one larger dataset and changing the distance column from miles to kilometers.\n",
    "\n",
    "-**get_taxi_html() and find_taxi_parquet_urls():** These functions are implementing web scraping in order to find the links for each data set of yellow taxi cabs. The first function, get_taxi_html(), is returning the html content of the web page that has the Taxi data. The second function, find_taxi_parquet_links is from the web page pulling out all of the links, then iterating through those links to see which are datasets for yellow taxi cabs from January 2009 to June 2015.\n",
    "\n",
    "-**get_and_clean_month_taxi_data(url)** This function reads in the data for each month of each year.\n",
    "\n",
    "-**get_and_clean_taxi_data()** This function concatinates all of the data for each month of each year into one large data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_html():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    # check if the request was succeeded\n",
    "    if not response.status_code == 200:\n",
    "        return None\n",
    "    return html\n",
    "\n",
    "\n",
    "def find_taxi_parquet_urls():\n",
    "    soup = bs4.BeautifulSoup(get_taxi_html(), 'html.parser')\n",
    "    yellow_pattern = r\"yellow_tripdata\"\n",
    "    # from Jan. 2009 to June 2015\n",
    "    year_pattern = r\"200\\d{1}\" # from Jan. 2009 to Dec. 2009\n",
    "    year_pattern2 = r\"201[01234]\" # from Jan. 2010 to Dec.2014\n",
    "    pattern2015 = r\"2015-0[123456]\" # from Jan. 2015 to June 2015\n",
    "    link_list = [a['href'] for a in soup.find_all('a')[30:-25]]\n",
    "    new_links = list()\n",
    "    for item in link_list:\n",
    "        # iterate through each year 2009 - 2015\n",
    "        if (re.search(yellow_pattern, item) != None): \n",
    "            if (re.search(year_pattern, item) != None):\n",
    "                new_links.append(item)\n",
    "            if (re.search(year_pattern2, item) != None):\n",
    "                new_links.append(item)\n",
    "            if (re.search(pattern2015, item) != None):\n",
    "                new_links.append(item)\n",
    "    return new_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    return pd.read_parquet(url,engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be57e277",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project Scaffolding.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# print(find_taxi_parquet_urls())\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_parquet\u001b[39m=\u001b[39m get_and_clean_month_taxi_data(\u001b[39m'\u001b[39;49m\u001b[39mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_parquet\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "\u001b[1;32m/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project Scaffolding.ipynb Cell 16\u001b[0m in \u001b[0;36mget_and_clean_month_taxi_data\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_and_clean_month_taxi_data\u001b[39m(url):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39;49mread_parquet(url,engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpyarrow\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[39mDataFrame\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    491\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    494\u001b[0m     path,\n\u001b[1;32m    495\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m    496\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    497\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    499\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parquet.py:233\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    231\u001b[0m     to_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39msplit_blocks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    234\u001b[0m     path,\n\u001b[1;32m    235\u001b[0m     kwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mfilesystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    236\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    237\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    238\u001b[0m )\n\u001b[1;32m    239\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[1;32m    241\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39mcolumns, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    242\u001b[0m     )\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[1;32m     95\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m    103\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:670\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    667\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    669\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[0;32m--> 670\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    671\u001b[0m     path_or_buf,\n\u001b[1;32m    672\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    673\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    674\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    675\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    676\u001b[0m )\n\u001b[1;32m    678\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    679\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:344\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    342\u001b[0m             \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n\u001b[1;32m    343\u001b[0m             compression \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m--> 344\u001b[0m         reader \u001b[39m=\u001b[39m BytesIO(req\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m    345\u001b[0m     \u001b[39mreturn\u001b[39;00m IOArgs(\n\u001b[1;32m    346\u001b[0m         filepath_or_buffer\u001b[39m=\u001b[39mreader,\n\u001b[1;32m    347\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         mode\u001b[39m=\u001b[39mfsspec_mode,\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:476\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength)\n\u001b[1;32m    477\u001b[0m     \u001b[39mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    478\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:626\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m s \u001b[39m=\u001b[39m []\n\u001b[1;32m    625\u001b[0m \u001b[39mwhile\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 626\u001b[0m     chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(\u001b[39mmin\u001b[39;49m(amt, MAXAMOUNT))\n\u001b[1;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunk:\n\u001b[1;32m    628\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(s), amt)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(find_taxi_parquet_urls())\n",
    "test_parquet= get_and_clean_month_taxi_data('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet')\n",
    "test_parquet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884672ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:11:33</td>\n",
       "      <td>2015-01-01 00:16:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1.609344</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>41</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:18:24</td>\n",
       "      <td>2015-01-01 00:24:20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.448410</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>166</td>\n",
       "      <td>238</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:26:19</td>\n",
       "      <td>2015-01-01 00:41:06</td>\n",
       "      <td>1</td>\n",
       "      <td>5.632704</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.40</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:45:26</td>\n",
       "      <td>2015-01-01 00:53:20</td>\n",
       "      <td>1</td>\n",
       "      <td>3.379622</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>162</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.87</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:59:21</td>\n",
       "      <td>2015-01-01 01:05:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.609344</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:07:31</td>\n",
       "      <td>2015-01-01 00:11:32</td>\n",
       "      <td>1</td>\n",
       "      <td>1.287475</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>238</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:47:08</td>\n",
       "      <td>2015-01-01 00:54:50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.770278</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:58:04</td>\n",
       "      <td>2015-01-01 01:11:56</td>\n",
       "      <td>1</td>\n",
       "      <td>4.667098</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:29:25</td>\n",
       "      <td>2015-01-01 00:37:25</td>\n",
       "      <td>2</td>\n",
       "      <td>2.092147</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>90</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:39:02</td>\n",
       "      <td>2015-01-01 01:02:37</td>\n",
       "      <td>2</td>\n",
       "      <td>6.920179</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>125</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2015-01-01 00:11:33   2015-01-01 00:16:48                1   \n",
       "1         1  2015-01-01 00:18:24   2015-01-01 00:24:20                1   \n",
       "2         1  2015-01-01 00:26:19   2015-01-01 00:41:06                1   \n",
       "3         1  2015-01-01 00:45:26   2015-01-01 00:53:20                1   \n",
       "4         1  2015-01-01 00:59:21   2015-01-01 01:05:24                1   \n",
       "5         1  2015-01-01 00:07:31   2015-01-01 00:11:32                1   \n",
       "6         1  2015-01-01 00:47:08   2015-01-01 00:54:50                1   \n",
       "7         1  2015-01-01 00:58:04   2015-01-01 01:11:56                1   \n",
       "8         1  2015-01-01 00:29:25   2015-01-01 00:37:25                2   \n",
       "9         1  2015-01-01 00:39:02   2015-01-01 01:02:37                2   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0       1.609344           1                  N            41           166   \n",
       "1       1.448410           1                  N           166           238   \n",
       "2       5.632704           1                  N           238           162   \n",
       "3       3.379622           1                  N           162           263   \n",
       "4       1.609344           1                  N           236           141   \n",
       "5       1.287475           1                  N           239           238   \n",
       "6       1.770278           1                  N           238           239   \n",
       "7       4.667098           1                  N           238            42   \n",
       "8       2.092147           1                  N            90           125   \n",
       "9       6.920179           1                  N           125           141   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1          5.7    0.5      0.5        1.40           0.0   \n",
       "1             3          6.0    0.5      0.5        0.00           0.0   \n",
       "2             1         13.2    0.5      0.5        2.90           0.0   \n",
       "3             1          8.2    0.5      0.5        2.37           0.0   \n",
       "4             3          6.0    0.5      0.5        0.00           0.0   \n",
       "5             2          5.0    0.5      0.5        0.00           0.0   \n",
       "6             2          7.0    0.5      0.5        0.00           0.0   \n",
       "7             1         12.2    0.5      0.5        2.70           0.0   \n",
       "8             2          7.0    0.5      0.5        0.00           0.0   \n",
       "9             2         18.0    0.5      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount congestion_surcharge airport_fee  \n",
       "0                    0.0          8.40                 None        None  \n",
       "1                    0.0          7.30                 None        None  \n",
       "2                    0.0         17.40                 None        None  \n",
       "3                    0.0         11.87                 None        None  \n",
       "4                    0.0          7.30                 None        None  \n",
       "5                    0.0          6.30                 None        None  \n",
       "6                    0.0          8.30                 None        None  \n",
       "7                    0.0         16.20                 None        None  \n",
       "8                    0.0          8.30                 None        None  \n",
       "9                    0.0         19.30                 None        None  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def miles_to_km(distance_miles):\n",
    "    distance_km = distance_miles /0.62137119\n",
    "    return distance_km\n",
    "\n",
    "test_parquet['trip_distance'] = test_parquet.apply(lambda x: miles_to_km(x[\"trip_distance\"]), axis = 1)\n",
    "\n",
    "\n",
    "test_parquet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zones_within_bbox():\n",
    "    #taxi_zones = pd.read_json('NYCTaxiZones.geojson')\n",
    "    import json\n",
    "    from shapely.geometry import Point\n",
    "    #taxi_zones = json.loads('taxizonedata.json')\n",
    "    #taxi_zones = taxi_zones.to_json(orient = 'columns')\n",
    "    \n",
    "    #taxi_zones= geopandas.read_file('NYCTaxiZones.geojson)\n",
    "    df = gpd.read_file('NYCTaxiZones.geojson')\n",
    "\n",
    "    #taxi_zones = gpd.GeoSeries(df)\n",
    "    taxi_zones = gpd.GeoDataFrame(df)\n",
    "    #df = df.ix[0]\n",
    "\n",
    "    taxi_zones = taxi_zones.to_crs(4326)\n",
    "\n",
    "    taxi_zones['lon'] = taxi_zones.centroid.x  \n",
    "    taxi_zones['lat'] = taxi_zones.centroid.y\n",
    "\n",
    "    northlimit  = 40.908524\n",
    "    southlimit = 40.560445\n",
    "    eastlimit = -73.717047\n",
    "    westlimit = -74.242330\n",
    "\n",
    "    \n",
    "    taxi_zones = taxi_zones[(taxi_zones[\"lon\"] <= eastlimit) & (taxi_zones[\"lon\"] >= westlimit)] \n",
    "    taxi_zones = taxi_zones[(taxi_zones[\"lat\"] <= northlimit) & (taxi_zones[\"lat\"]>= southlimit)]\n",
    "\n",
    "    return taxi_zones\n",
    "\n",
    "\n",
    "def get_and_clean_taxi_data():\n",
    "    # NOT OUR CODE\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_parquet_urls()\n",
    "    for csv_url in all_csv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "\n",
    "        # Making sure that the zone is in the [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "        #dataframe = dataframe[(dataframe['PULocationID']== zone) & (dataframe['DOLocationID']== zone)]\n",
    "\n",
    "        # Changing the distance column of each data set from miles to kilometers\n",
    "        dataframe['trip_distance'] = dataframe.apply(lambda x: x[\"trip_distance\"]/0.62137119, axis = 1)\n",
    "        \n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data\n",
    "    # NOT OUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a4f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_93745/1795060068.py:17: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  taxi_zones['lon'] = taxi_zones.centroid.x\n",
      "/var/folders/bx/vxcv_d0d4mvb9fjsgycq43yr0000gn/T/ipykernel_93745/1795060068.py:18: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  taxi_zones['lat'] = taxi_zones.centroid.y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_area</th>\n",
       "      <th>objectid</th>\n",
       "      <th>shape_leng</th>\n",
       "      <th>location_id</th>\n",
       "      <th>zone</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0007823067885</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116357453189</td>\n",
       "      <td>1</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "      <td>MULTIPOLYGON (((-74.18445 40.69500, -74.18449 ...</td>\n",
       "      <td>-74.174000</td>\n",
       "      <td>40.691831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00486634037837</td>\n",
       "      <td>2</td>\n",
       "      <td>0.43346966679</td>\n",
       "      <td>2</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...</td>\n",
       "      <td>-73.831299</td>\n",
       "      <td>40.616745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000314414156821</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0843411059012</td>\n",
       "      <td>3</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>MULTIPOLYGON (((-73.84793 40.87134, -73.84725 ...</td>\n",
       "      <td>-73.847422</td>\n",
       "      <td>40.864474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000111871946192</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0435665270921</td>\n",
       "      <td>4</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MULTIPOLYGON (((-73.97177 40.72582, -73.97179 ...</td>\n",
       "      <td>-73.976968</td>\n",
       "      <td>40.723752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000606460984581</td>\n",
       "      <td>6</td>\n",
       "      <td>0.150490542523</td>\n",
       "      <td>6</td>\n",
       "      <td>Arrochar/Fort Wadsworth</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>MULTIPOLYGON (((-74.06367 40.60220, -74.06351 ...</td>\n",
       "      <td>-74.071771</td>\n",
       "      <td>40.600324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.000168611097013</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0679149669603</td>\n",
       "      <td>256</td>\n",
       "      <td>Williamsburg (South Side)</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.95834 40.71331, -73.95681 ...</td>\n",
       "      <td>-73.959905</td>\n",
       "      <td>40.710880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.000394552487366</td>\n",
       "      <td>259</td>\n",
       "      <td>0.126750305191</td>\n",
       "      <td>259</td>\n",
       "      <td>Woodlawn/Wakefield</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>MULTIPOLYGON (((-73.85107 40.91037, -73.85207 ...</td>\n",
       "      <td>-73.852215</td>\n",
       "      <td>40.897932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.000422345326907</td>\n",
       "      <td>260</td>\n",
       "      <td>0.133514154636</td>\n",
       "      <td>260</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((-73.90175 40.76078, -73.90147 ...</td>\n",
       "      <td>-73.906306</td>\n",
       "      <td>40.744235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.0000343423231652</td>\n",
       "      <td>261</td>\n",
       "      <td>0.0271204563616</td>\n",
       "      <td>261</td>\n",
       "      <td>World Trade Center</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MULTIPOLYGON (((-74.01333 40.70503, -74.01327 ...</td>\n",
       "      <td>-74.013023</td>\n",
       "      <td>40.709139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.000122330270966</td>\n",
       "      <td>262</td>\n",
       "      <td>0.0490636231541</td>\n",
       "      <td>262</td>\n",
       "      <td>Yorkville East</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MULTIPOLYGON (((-73.94383 40.78286, -73.94376 ...</td>\n",
       "      <td>-73.946510</td>\n",
       "      <td>40.775932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             shape_area objectid       shape_leng location_id  \\\n",
       "0       0.0007823067885        1   0.116357453189           1   \n",
       "1      0.00486634037837        2    0.43346966679           2   \n",
       "2     0.000314414156821        3  0.0843411059012           3   \n",
       "3     0.000111871946192        4  0.0435665270921           4   \n",
       "5     0.000606460984581        6   0.150490542523           6   \n",
       "..                  ...      ...              ...         ...   \n",
       "258   0.000168611097013      256  0.0679149669603         256   \n",
       "259   0.000394552487366      259   0.126750305191         259   \n",
       "260   0.000422345326907      260   0.133514154636         260   \n",
       "261  0.0000343423231652      261  0.0271204563616         261   \n",
       "262   0.000122330270966      262  0.0490636231541         262   \n",
       "\n",
       "                          zone        borough  \\\n",
       "0               Newark Airport            EWR   \n",
       "1                  Jamaica Bay         Queens   \n",
       "2      Allerton/Pelham Gardens          Bronx   \n",
       "3                Alphabet City      Manhattan   \n",
       "5      Arrochar/Fort Wadsworth  Staten Island   \n",
       "..                         ...            ...   \n",
       "258  Williamsburg (South Side)       Brooklyn   \n",
       "259         Woodlawn/Wakefield          Bronx   \n",
       "260                   Woodside         Queens   \n",
       "261         World Trade Center      Manhattan   \n",
       "262             Yorkville East      Manhattan   \n",
       "\n",
       "                                              geometry        lon        lat  \n",
       "0    MULTIPOLYGON (((-74.18445 40.69500, -74.18449 ... -74.174000  40.691831  \n",
       "1    MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ... -73.831299  40.616745  \n",
       "2    MULTIPOLYGON (((-73.84793 40.87134, -73.84725 ... -73.847422  40.864474  \n",
       "3    MULTIPOLYGON (((-73.97177 40.72582, -73.97179 ... -73.976968  40.723752  \n",
       "5    MULTIPOLYGON (((-74.06367 40.60220, -74.06351 ... -74.071771  40.600324  \n",
       "..                                                 ...        ...        ...  \n",
       "258  MULTIPOLYGON (((-73.95834 40.71331, -73.95681 ... -73.959905  40.710880  \n",
       "259  MULTIPOLYGON (((-73.85107 40.91037, -73.85207 ... -73.852215  40.897932  \n",
       "260  MULTIPOLYGON (((-73.90175 40.76078, -73.90147 ... -73.906306  40.744235  \n",
       "261  MULTIPOLYGON (((-74.01333 40.70503, -74.01327 ... -74.013023  40.709139  \n",
       "262  MULTIPOLYGON (((-73.94383 40.78286, -73.94376 ... -73.946510  40.775932  \n",
       "\n",
       "[255 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones_within_bbox()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff321bec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "In this portion of the project, two functions load_and_clean_uber_data() and get_uber_data() are used.  The function load_and_clean_uber_data() reads in the uber data from a csv file and returns a dataframe.  The function get_uber_data() uses the previous function to read in the data, then uses the function add_distance_column previously defined in order to add the distance in kilometers of each trip taken by an uber in the dataset and returns the uber data as a dataframe.  In addition, some additional processing was done ot the data to convert the pickup_datetime into a datetime object and create a column for the day of the week the pickup occured on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    return pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    # Making Pickup_datetime a datetime object\n",
    "    df[\"pickup_datetime\"]  = df[\"pickup_datetime\"].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S %Z'))\n",
    "    df[\"day_of_week\"]= df['pickup_datetime'].apply(lambda x: x.isoweekday())\n",
    "\n",
    "    # Removing any data outside of the [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    #NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "    northlimit  = 40.908524\n",
    "    southlimit = 40.560445\n",
    "    eastlimit = -73.717047\n",
    "    westlimit = -74.242330\n",
    "    \n",
    "    uber_dataframe = uber_dataframe[(uber_dataframe[\"pickup_longitude\"] <= eastlimit) & (uber_dataframe[\"pickup_longitude\"] >= westlimit)] \n",
    "    uber_dataframe = uber_dataframe [(uber_dataframe [\"pickup_latitude\"] <= northlimit) & (uber_dataframe[\"pickup_latitude\"]>= southlimit)]\n",
    "\n",
    "    uber_dataframe  = uber_dataframe[(uber_dataframe[\"dropoff_longitude\"] <= eastlimit) & (uber_dataframe[\"dropoff_longitude\"] >= westlimit)] \n",
    "    uber_dataframe = uber_dataframe[(uber_dataframe[\"dropoff_latitude\"] <= northlimit) & (uber_dataframe[\"dropoff_latitude\"]>= southlimit)]\n",
    "\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    # Just need to read in the data as a measurment is taken each hour \n",
    "    all_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # split data hourly\n",
    "    date = all_data['DATE']\n",
    "    import datetime\n",
    "    date = date.apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%dT%H:%M:%S'))\n",
    "    all_data['hours'] = date.apply(lambda x:x.hour)\n",
    "    all_data['newDATE'] = 0\n",
    "    for i in range(len(date)):\n",
    "        all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
    "    # now we have cleaned the data, now rename it\n",
    "    hourly_data = all_data\n",
    "\n",
    "    # you'll find which way to use in the later part\n",
    "    # combine them all together and split into 24 rows\n",
    "    # return clean_month_weather_data_hourly_all.drop_duplicates(subset=['hours'])\n",
    "\n",
    "    # split values into 24 rows for each day\n",
    "    return hourly_data.drop_duplicates(subset=['hours', 'newDATE'],keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    #need to combine rows for each given day\n",
    "    all_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # split data daily\n",
    "    date = all_data['DATE']\n",
    "    import datetime\n",
    "    date = date.apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%dT%H:%M:%S'))\n",
    "    all_data['days'] = date.apply(lambda x:x.day)\n",
    "    all_data['newDATE'] = 0\n",
    "    for i in range(len(date)):\n",
    "        all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
    "    # now we have cleaned the data, now rename it\n",
    "    daily_data = all_data\n",
    "\n",
    "    # you'll find which way to use in the later part\n",
    "    # combine them all together and split into 31 rows\n",
    "    # return clean_month_weather_data_hourly_all.drop_duplicates(subset=['hours'])\n",
    "\n",
    "    # split values into 31/30 rows for each month\n",
    "    return daily_data.drop_duplicates(subset=['days', 'newDATE'],keep='last')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "\n",
    "    # add this at the beginnig\n",
    "    # weather csv data file\n",
    "    # 2009_weather to 2015_weather (just pick the first 6 months)\n",
    "    csv__= '/Users/morax/Documents/哥大/IEORE4501/IEOR4501 HW/IEOR4501 Project/'\n",
    "    csv09_file = csv__ + '2009_weather.csv'\n",
    "    csv10_file = csv__ + '2010_weather.csv'\n",
    "    csv11_file = csv__ + '2011_weather.csv'\n",
    "    csv12_file = csv__ + '2012_weather.csv'\n",
    "    csv13_file = csv__ + '2013_weather.csv'\n",
    "    csv14_file = csv__ + '2014_weather.csv'\n",
    "    csv15_file = csv__ + '2015_weather.csv'\n",
    "    weather_csv09_14_files = [csv09_file, csv10_file, csv11_file, csv12_file, csv13_file, csv14_file]\n",
    "    \n",
    "    for csv_file in weather_csv09_14_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "    hourly_15_dataframe = clean_month_weather_data_hourly(csv15_file).iloc[:4344]\n",
    "    hourly_dataframes.append(hourly_15_dataframe)\n",
    "    daily_15_dataframe = clean_month_weather_data_daily(csv15_file).iloc[:181]\n",
    "    daily_dataframes.append(daily_15_dataframe)\n",
    "\n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes).reset_index(drop=True)\n",
    "    daily_data = pd.concat(daily_dataframes).reset_index(drop=True)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "Once all of the functions in order to process the data have been written each of those functions can be executed.  Executing each of these functions, provides four clean data sets, taxi_data, uber_data, hourly_weather_data, and daily_weather_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:3: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:3: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:3: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:3: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:3: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:3: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:3: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:3: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:3: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:3: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:3: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/989517460.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:10]\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:3: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.read_csv(csv_file)\n",
      "/var/folders/0l/chsxlz0n6ld25hkms7fljx0c0000gn/T/ipykernel_3500/3229008894.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['newDATE'][i] = all_data['DATE'][i][:7]\n"
     ]
    }
   ],
   "source": [
    "#taxi_data = get_and_clean_taxi_data()\n",
    "# uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37437cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "843c2ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>REPORT_TYPE</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>HourlyAltimeterSetting</th>\n",
       "      <th>HourlyDewPointTemperature</th>\n",
       "      <th>...</th>\n",
       "      <th>BackupDistanceUnit</th>\n",
       "      <th>BackupElements</th>\n",
       "      <th>BackupElevation</th>\n",
       "      <th>BackupEquipment</th>\n",
       "      <th>BackupLatitude</th>\n",
       "      <th>BackupLongitude</th>\n",
       "      <th>BackupName</th>\n",
       "      <th>WindEquipmentChangeDate</th>\n",
       "      <th>days</th>\n",
       "      <th>newDATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-01T23:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>4</td>\n",
       "      <td>30.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-02T23:59:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>SOD</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>2</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-03T23:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>4</td>\n",
       "      <td>30.08</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-04T23:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>4</td>\n",
       "      <td>30.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>4</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-05T23:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>4</td>\n",
       "      <td>30.03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-06T23:59:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>SOD</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>6</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-07T23:59:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>SOD</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>7</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-08T23:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>4</td>\n",
       "      <td>29.73</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>8</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-09T23:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>4</td>\n",
       "      <td>30.26</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>9</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2009-01-10T23:59:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>SOD</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>mi</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK ZOO</td>\n",
       "      <td>2006-09-18</td>\n",
       "      <td>10</td>\n",
       "      <td>2009-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION                 DATE  LATITUDE  LONGITUDE  ELEVATION  \\\n",
       "0  72505394728  2009-01-01T23:51:00  40.77898  -73.96925       42.7   \n",
       "1  72505394728  2009-01-02T23:59:00  40.77898  -73.96925       42.7   \n",
       "2  72505394728  2009-01-03T23:51:00  40.77898  -73.96925       42.7   \n",
       "3  72505394728  2009-01-04T23:51:00  40.77898  -73.96925       42.7   \n",
       "4  72505394728  2009-01-05T23:51:00  40.77898  -73.96925       42.7   \n",
       "5  72505394728  2009-01-06T23:59:00  40.77898  -73.96925       42.7   \n",
       "6  72505394728  2009-01-07T23:59:00  40.77898  -73.96925       42.7   \n",
       "7  72505394728  2009-01-08T23:51:00  40.77898  -73.96925       42.7   \n",
       "8  72505394728  2009-01-09T23:51:00  40.77898  -73.96925       42.7   \n",
       "9  72505394728  2009-01-10T23:59:00  40.77898  -73.96925       42.7   \n",
       "\n",
       "                          NAME REPORT_TYPE SOURCE HourlyAltimeterSetting  \\\n",
       "0  NY CITY CENTRAL PARK, NY US       AUTO       4                  30.22   \n",
       "1  NY CITY CENTRAL PARK, NY US       SOD        O                    NaN   \n",
       "2  NY CITY CENTRAL PARK, NY US       AUTO       4                  30.08   \n",
       "3  NY CITY CENTRAL PARK, NY US       AUTO       4                  30.00   \n",
       "4  NY CITY CENTRAL PARK, NY US       AUTO       4                  30.03   \n",
       "5  NY CITY CENTRAL PARK, NY US       SOD        O                    NaN   \n",
       "6  NY CITY CENTRAL PARK, NY US       SOD        O                    NaN   \n",
       "7  NY CITY CENTRAL PARK, NY US       AUTO       4                  29.73   \n",
       "8  NY CITY CENTRAL PARK, NY US       AUTO       4                  30.26   \n",
       "9  NY CITY CENTRAL PARK, NY US       SOD        O                    NaN   \n",
       "\n",
       "  HourlyDewPointTemperature  ... BackupDistanceUnit BackupElements  \\\n",
       "0                       1.0  ...                 mi           SNOW   \n",
       "1                       NaN  ...                 mi           SNOW   \n",
       "2                       9.0  ...                 mi           SNOW   \n",
       "3                      12.0  ...                 mi           SNOW   \n",
       "4                       3.0  ...                 mi           SNOW   \n",
       "5                       NaN  ...                 mi           SNOW   \n",
       "6                       NaN  ...                 mi           SNOW   \n",
       "7                      16.0  ...                 mi           SNOW   \n",
       "8                      12.0  ...                 mi           SNOW   \n",
       "9                       NaN  ...                 mi           SNOW   \n",
       "\n",
       "  BackupElevation BackupEquipment  BackupLatitude  BackupLongitude  \\\n",
       "0             NaN       SNOWBOARD             NaN              NaN   \n",
       "1             NaN       SNOWBOARD             NaN              NaN   \n",
       "2             NaN       SNOWBOARD             NaN              NaN   \n",
       "3             NaN       SNOWBOARD             NaN              NaN   \n",
       "4             NaN       SNOWBOARD             NaN              NaN   \n",
       "5             NaN       SNOWBOARD             NaN              NaN   \n",
       "6             NaN       SNOWBOARD             NaN              NaN   \n",
       "7             NaN       SNOWBOARD             NaN              NaN   \n",
       "8             NaN       SNOWBOARD             NaN              NaN   \n",
       "9             NaN       SNOWBOARD             NaN              NaN   \n",
       "\n",
       "         BackupName WindEquipmentChangeDate days  newDATE  \n",
       "0  CENTRAL PARK ZOO              2006-09-18    1  2009-01  \n",
       "1  CENTRAL PARK ZOO              2006-09-18    2  2009-01  \n",
       "2  CENTRAL PARK ZOO              2006-09-18    3  2009-01  \n",
       "3  CENTRAL PARK ZOO              2006-09-18    4  2009-01  \n",
       "4  CENTRAL PARK ZOO              2006-09-18    5  2009-01  \n",
       "5  CENTRAL PARK ZOO              2006-09-18    6  2009-01  \n",
       "6  CENTRAL PARK ZOO              2006-09-18    7  2009-01  \n",
       "7  CENTRAL PARK ZOO              2006-09-18    8  2009-01  \n",
       "8  CENTRAL PARK ZOO              2006-09-18    9  2009-01  \n",
       "9  CENTRAL PARK ZOO              2006-09-18   10  2009-01  \n",
       "\n",
       "[10 rows x 125 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head(10)\n",
    "daily_weather_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "_Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)\n",
    "# First, using SQLAlchemy, create a SQLite database with which you’ll load in your preprocessed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "    (id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    LATITUDE INTEGER,\n",
    "    LONGITUDE INTEGER,\n",
    "    NAME TEXT,\n",
    "    BackupName TEXT,\n",
    "    hours INTEGER,\n",
    "    newDATE DATE)\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "    (id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    LATITUDE INTEGER,\n",
    "    LONGITUDE INTEGER,\n",
    "    NAME TEXT,\n",
    "    BackupName TEXT,\n",
    "    days INTEGER,\n",
    "    newDATE DATE)\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    engine.connect().execute(\n",
    "        HOURLY_WEATHER_SCHEMA\n",
    "    )\n",
    "    engine.connect().execute(\n",
    "        DAILY_WEATHER_SCHEMA\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f527aa4",
   "metadata": {},
   "source": [
    "pd.read_sql_query - read data from querying a SQL table\n",
    "pd.read_sql_table - read entire SQL table\n",
    "df.to_sql - add data from the dataframe to a SQL table\n",
    "pd.to_numeric - Convert argument to a numeric type\n",
    "pd.concat - Concatenate pandas objects along a particular axis with optional set logic along the other axes\n",
    "pd.merge - Merge DataFrame or named Series objects with a database-style join\n",
    "pd.merge_asof - Perform a merge by key distance. This is similar to a left-join except that we match on the nearest key rather than equal keys. Both DataFrames must be sorted by the key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    # sessionmaker returns a Session class\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    # and we create an instance of Session\n",
    "    session = Session()\n",
    "    session.add_all(table_to_df_dict) # add multiple entries at once\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    # \"taxi_trips\": taxi_data,\n",
    "    # \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74004f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnmappedInstanceError",
     "evalue": "Class 'builtins.str' is not mapped",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/orm/session.py:2599\u001b[0m, in \u001b[0;36mSession.add\u001b[0;34m(self, instance, _warn)\u001b[0m\n\u001b[1;32m   2598\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2599\u001b[0m     state \u001b[39m=\u001b[39m attributes\u001b[39m.\u001b[39;49minstance_state(instance)\n\u001b[1;32m   2600\u001b[0m \u001b[39mexcept\u001b[39;00m exc\u001b[39m.\u001b[39mNO_STATE \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_sa_instance_state'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mUnmappedInstanceError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project Scaffolding.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m write_dataframes_to_table(map_table_name_to_dataframe)\n",
      "\u001b[1;32m/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project Scaffolding.ipynb Cell 42\u001b[0m in \u001b[0;36mwrite_dataframes_to_table\u001b[0;34m(table_to_df_dict)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# and we create an instance of Session\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m session \u001b[39m=\u001b[39m Session()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m session\u001b[39m.\u001b[39;49madd_all(table_to_df_dict) \u001b[39m# add multiple entries at once\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morax/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m session\u001b[39m.\u001b[39mcommit()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/orm/session.py:2615\u001b[0m, in \u001b[0;36mSession.add_all\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m   2612\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flush_warning(\u001b[39m\"\u001b[39m\u001b[39mSession.add_all()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2614\u001b[0m \u001b[39mfor\u001b[39;00m instance \u001b[39min\u001b[39;00m instances:\n\u001b[0;32m-> 2615\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd(instance, _warn\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/orm/session.py:2601\u001b[0m, in \u001b[0;36mSession.add\u001b[0;34m(self, instance, _warn)\u001b[0m\n\u001b[1;32m   2599\u001b[0m     state \u001b[39m=\u001b[39m attributes\u001b[39m.\u001b[39minstance_state(instance)\n\u001b[1;32m   2600\u001b[0m \u001b[39mexcept\u001b[39;00m exc\u001b[39m.\u001b[39mNO_STATE \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 2601\u001b[0m     util\u001b[39m.\u001b[39;49mraise_(\n\u001b[1;32m   2602\u001b[0m         exc\u001b[39m.\u001b[39;49mUnmappedInstanceError(instance),\n\u001b[1;32m   2603\u001b[0m         replace_context\u001b[39m=\u001b[39;49merr,\n\u001b[1;32m   2604\u001b[0m     )\n\u001b[1;32m   2606\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_or_update_state(state)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/util/compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    204\u001b[0m     exception\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m replace_context\n\u001b[1;32m    206\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[1;32m    208\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[39mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "\u001b[0;31mUnmappedInstanceError\u001b[0m: Class 'builtins.str' is not mapped"
     ]
    }
   ],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "\n",
    "\n",
    "\n",
    "    raise NotImplemented()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142954a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day_of_week\n",
       "5    30880\n",
       "6    30251\n",
       "4    30021\n",
       "3    29037\n",
       "2    28127\n",
       "7    26441\n",
       "1    25243\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def uber_popular_day_of_week(df = uber_data):\n",
    "     \n",
    "     day_of_week_group = df.groupby('day_of_week').size().sort_values(ascending=False)\n",
    "     return day_of_week_group\n",
    "\n",
    "uber_popular_day_of_week()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652192d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taxi_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/laurelhickey/Documents/GitHub/Tools_for_Analytics_Project/Project Scaffolding.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laurelhickey/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtaxi_most_pickup_hour\u001b[39m(df \u001b[39m=\u001b[39m taxi_data):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurelhickey/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mtpep_dropoff_datetime\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mtpep_dropoff_datetime\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mstrptime(x,\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurelhickey/Documents/GitHub/Tools_for_Analytics_Project/Project%20Scaffolding.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mtpep_pickup_datetime\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mtpep_pickup_datetime\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mstrptime(x,\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taxi_data' is not defined"
     ]
    }
   ],
   "source": [
    "def taxi_most_pickup_hour(df = taxi_data):\n",
    "    df[\"tpep_dropoff_datetime\"]  = df[\"tpep_dropoff_datetime\"].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "    df[\"tpep_pickup_datetime\"]  = df[\"tpep_pickup_datetime\"].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    df['pickup_hour'] = df['tpep_pickup_datetime'].apply(lambda x:x.hour)\n",
    "\n",
    "    pickup_hour_group = df.groupby(['pickup_hour']).size().sort_values(ascending=False)\n",
    "\n",
    "    return pickup_hour_group\n",
    "\n",
    "\n",
    "taxi_most_pickup_hour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_N = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_N).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_N, \"some_descriptive_name.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51149848",
   "metadata": {},
   "source": [
    "### Visualization: For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? \n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016671a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_hour_of_day(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9877d",
   "metadata": {},
   "source": [
    "### Visualization: Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR. Compares what day of the week was most popular for drop offs for each airport.\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "\n",
    "EWR (Newark): -74.195995,40.664103,-74.148445,40.713045\n",
    "\n",
    "JFK: -73.832496,40.618362,-73.744262,40.669421\n",
    "\n",
    "LGA (LaGuardia): -73.892010,40.764638,-73.852357,40.787711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be fixed up for our purposes\n",
    "# Just looking at uber data?????\n",
    "def get_zone(lon,lat,airport_boxes):\n",
    "    \n",
    "    #MY CODE STARTS HERE\n",
    "    # long is x lat is y \n",
    "    \n",
    "    for item in airport_boxes:\n",
    "        #print(item)\n",
    "        if (item[1][0][0] <= lon <= item[1][1][0]) & (item[1][0][1] <= lat <= item[1][2][1]):\n",
    "            zone = item[0]\n",
    "            # want to return airport name \n",
    "        else:\n",
    "            # want to return na if no airport name\n",
    "            break\n",
    "\n",
    "df['dropoff_zone'] = df.apply(lambda x: get_zone(x[\"dropoff_longitude\"], x[\"dropoff_latitude\"], zone_table),axis =1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_hour_of_day(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd695dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595367c",
   "metadata": {},
   "source": [
    "### Visualization:  A heatmap of all hired trips over a map of the area\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf53e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_hour_of_day(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19657f",
   "metadata": {},
   "source": [
    "### Visualization: The average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528da4c7",
   "metadata": {},
   "source": [
    "### Visualization: A scatter plot that compares tip amount versus distance\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_hour_of_day(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f240076",
   "metadata": {},
   "source": [
    "### Visualization: A scatter plot that compares tip amount versus precipitation amount\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c011ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_hour_of_day(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103741b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2aa6e1aac2aa7bfc589de973ebfd8def4ef3bade44a54773dc50a33bd91aaacb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
